{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras.backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Democrat</th>\n",
       "      <th>population</th>\n",
       "      <th>population_change</th>\n",
       "      <th>age65plus</th>\n",
       "      <th>Black</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Edu_bachelors</th>\n",
       "      <th>income</th>\n",
       "      <th>Poverty</th>\n",
       "      <th>Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>55395</td>\n",
       "      <td>1.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.027</td>\n",
       "      <td>20.9</td>\n",
       "      <td>53682</td>\n",
       "      <td>12.1</td>\n",
       "      <td>91.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8433</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.028</td>\n",
       "      <td>19.9</td>\n",
       "      <td>51793</td>\n",
       "      <td>9.1</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>15080</td>\n",
       "      <td>0.7</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.0</td>\n",
       "      <td>38019</td>\n",
       "      <td>18.4</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>29317</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.113</td>\n",
       "      <td>13.7</td>\n",
       "      <td>39267</td>\n",
       "      <td>20.6</td>\n",
       "      <td>23.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>68838</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.058</td>\n",
       "      <td>12.5</td>\n",
       "      <td>33159</td>\n",
       "      <td>22.7</td>\n",
       "      <td>91.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>14.8</td>\n",
       "      <td>44149</td>\n",
       "      <td>16.9</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.034</td>\n",
       "      <td>15.9</td>\n",
       "      <td>37607</td>\n",
       "      <td>39.3</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>163820</td>\n",
       "      <td>1.4</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.027</td>\n",
       "      <td>36.0</td>\n",
       "      <td>58080</td>\n",
       "      <td>13.9</td>\n",
       "      <td>390.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>66600</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.041</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36334</td>\n",
       "      <td>21.5</td>\n",
       "      <td>120.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>21362</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>18.7</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.030</td>\n",
       "      <td>20.0</td>\n",
       "      <td>53057</td>\n",
       "      <td>8.3</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>4508</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>12.4</td>\n",
       "      <td>19986</td>\n",
       "      <td>37.7</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.7</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.025</td>\n",
       "      <td>16.4</td>\n",
       "      <td>71250</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>8996</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>14.3</td>\n",
       "      <td>42025</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>44943</td>\n",
       "      <td>5.2</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.219</td>\n",
       "      <td>21.9</td>\n",
       "      <td>48115</td>\n",
       "      <td>16.3</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>6679</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.037</td>\n",
       "      <td>15.4</td>\n",
       "      <td>64574</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>6598</td>\n",
       "      <td>2.9</td>\n",
       "      <td>19.6</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.022</td>\n",
       "      <td>19.7</td>\n",
       "      <td>48911</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>435286</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.067</td>\n",
       "      <td>23.4</td>\n",
       "      <td>41556</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1296.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>15994</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.015</td>\n",
       "      <td>13.8</td>\n",
       "      <td>37388</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>86697</td>\n",
       "      <td>3.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.023</td>\n",
       "      <td>15.5</td>\n",
       "      <td>50786</td>\n",
       "      <td>15.2</td>\n",
       "      <td>132.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>213869</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.064</td>\n",
       "      <td>26.1</td>\n",
       "      <td>60781</td>\n",
       "      <td>11.2</td>\n",
       "      <td>633.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Democrat  population  population_change  age65plus  Black  Hispanic  \\\n",
       "0          0       55395                1.5       13.8  0.187     0.027   \n",
       "1          0        8433               -2.0       19.3  0.008     0.028   \n",
       "2          0       15080                0.7       18.4  0.036     0.025   \n",
       "3          0       29317               -1.0       15.6  0.030     0.113   \n",
       "4          0       68838               -0.7       19.9  0.045     0.058   \n",
       "5          0       20652                1.0       17.6  0.022     0.033   \n",
       "6          0        3430                0.0       12.3  0.004     0.034   \n",
       "7          0      163820                1.4       15.6  0.073     0.027   \n",
       "8          0       66600               -1.8       19.5  0.099     0.041   \n",
       "9          0       21362               -1.4       18.7  0.006     0.030   \n",
       "10         0        4508               -5.2       17.8  0.005     0.010   \n",
       "11         0        1850                0.2       18.7  0.004     0.025   \n",
       "12         0        8996               -2.7       24.6  0.005     0.020   \n",
       "13         0       44943                5.2       21.1  0.024     0.219   \n",
       "14         0        6679               -2.6       15.0  0.041     0.037   \n",
       "15         0        6598                2.9       19.6  0.007     0.022   \n",
       "16         1      435286               -1.5       14.6  0.197     0.067   \n",
       "17         0       15994               -1.7       18.3  0.291     0.015   \n",
       "18         0       86697                3.7       15.3  0.095     0.023   \n",
       "19         1      213869                4.9       10.4  0.406     0.064   \n",
       "\n",
       "    Edu_bachelors  income  Poverty  Density  \n",
       "0            20.9   53682     12.1     91.8  \n",
       "1            19.9   51793      9.1     13.7  \n",
       "2             9.0   38019     18.4     26.7  \n",
       "3            13.7   39267     20.6     23.2  \n",
       "4            12.5   33159     22.7     91.9  \n",
       "5            14.8   44149     16.9     20.7  \n",
       "6            15.9   37607     39.3      2.9  \n",
       "7            36.0   58080     13.9    390.5  \n",
       "8            17.0   36334     21.5    120.2  \n",
       "9            20.0   53057      8.3     41.4  \n",
       "10           12.4   19986     37.7     24.1  \n",
       "11           16.4   71250      7.4      2.6  \n",
       "12           14.3   42025      9.0     13.3  \n",
       "13           21.9   48115     16.3     43.0  \n",
       "14           15.4   64574      8.7      5.3  \n",
       "15           19.7   48911     13.2      4.3  \n",
       "16           23.4   41556     20.8   1296.2  \n",
       "17           13.8   37388     20.6     28.6  \n",
       "18           15.5   50786     15.2    132.3  \n",
       "19           26.1   60781     11.2    633.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"votes-train.csv\")\n",
    "df_test = pd.read_csv(\"votes-test.csv\")\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing training and testing inputs and target matrices\n",
    "#using all features\n",
    "xtr = np.array(df_train[df_train.columns[1:]])\n",
    "ytr = np.array(df_train['Democrat'])\n",
    "xts = np.array(df_test[df_test.columns[1:]])\n",
    "yts = np.array(df_test['Democrat'])\n",
    "\n",
    "\n",
    "#feature scaling of input matrices xtr, xts\n",
    "xtr = (xtr - np.mean(xtr, axis=0))/(np.std(xtr, axis=0))\n",
    "xts = (xts - np.mean(xts, axis=0))/(np.std(xts, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 1000)              10000     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 12,002\n",
      "Trainable params: 12,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "nh=1000             #number of hidden units\n",
    "N = xtr.shape[1]  #number of features, aka number of input neurons\n",
    "M = 2             #number of output neurons. \n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(N,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(M, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2334 samples, validate on 777 samples\n",
      "Epoch 1/291\n",
      "2334/2334 [==============================] - 0s 50us/step - loss: 0.1623 - acc: 0.9310 - val_loss: 0.1861 - val_acc: 0.9266\n",
      "Epoch 2/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1589 - acc: 0.9319 - val_loss: 0.1799 - val_acc: 0.9228\n",
      "Epoch 3/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1568 - acc: 0.9366 - val_loss: 0.1848 - val_acc: 0.9228\n",
      "Epoch 4/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1562 - acc: 0.9366 - val_loss: 0.1819 - val_acc: 0.9305\n",
      "Epoch 5/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1623 - acc: 0.9336 - val_loss: 0.1859 - val_acc: 0.9241\n",
      "Epoch 6/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1625 - acc: 0.9319 - val_loss: 0.1792 - val_acc: 0.9266\n",
      "Epoch 7/291\n",
      "2334/2334 [==============================] - 0s 55us/step - loss: 0.1718 - acc: 0.9306 - val_loss: 0.1958 - val_acc: 0.9163\n",
      "Epoch 8/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1593 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9228\n",
      "Epoch 9/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1560 - acc: 0.9370 - val_loss: 0.1814 - val_acc: 0.9254\n",
      "Epoch 10/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1571 - acc: 0.9370 - val_loss: 0.1806 - val_acc: 0.9254\n",
      "Epoch 11/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1574 - acc: 0.9353 - val_loss: 0.1816 - val_acc: 0.9215\n",
      "Epoch 12/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1568 - acc: 0.9374 - val_loss: 0.2022 - val_acc: 0.9138\n",
      "Epoch 13/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1597 - acc: 0.9336 - val_loss: 0.1800 - val_acc: 0.9202\n",
      "Epoch 14/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1538 - acc: 0.9362 - val_loss: 0.1790 - val_acc: 0.9215\n",
      "Epoch 15/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1555 - acc: 0.9366 - val_loss: 0.1790 - val_acc: 0.9241\n",
      "Epoch 16/291\n",
      "2334/2334 [==============================] - 0s 50us/step - loss: 0.1546 - acc: 0.9374 - val_loss: 0.1826 - val_acc: 0.9279\n",
      "Epoch 17/291\n",
      "2334/2334 [==============================] - 0s 49us/step - loss: 0.1530 - acc: 0.9379 - val_loss: 0.1949 - val_acc: 0.9138\n",
      "Epoch 18/291\n",
      "2334/2334 [==============================] - 0s 50us/step - loss: 0.1551 - acc: 0.9366 - val_loss: 0.1882 - val_acc: 0.9202\n",
      "Epoch 19/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1540 - acc: 0.9362 - val_loss: 0.1792 - val_acc: 0.9228\n",
      "Epoch 20/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1540 - acc: 0.9366 - val_loss: 0.1780 - val_acc: 0.9241\n",
      "Epoch 21/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1537 - acc: 0.9374 - val_loss: 0.1792 - val_acc: 0.9254\n",
      "Epoch 22/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1527 - acc: 0.9357 - val_loss: 0.1782 - val_acc: 0.9254\n",
      "Epoch 23/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1544 - acc: 0.936 - 0s 43us/step - loss: 0.1533 - acc: 0.9404 - val_loss: 0.1860 - val_acc: 0.9241\n",
      "Epoch 24/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1559 - acc: 0.9362 - val_loss: 0.1890 - val_acc: 0.9189\n",
      "Epoch 25/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1638 - acc: 0.932 - 0s 41us/step - loss: 0.1548 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9266\n",
      "Epoch 26/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1539 - acc: 0.9379 - val_loss: 0.1782 - val_acc: 0.9215\n",
      "Epoch 27/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1567 - acc: 0.9362 - val_loss: 0.1847 - val_acc: 0.9254\n",
      "Epoch 28/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1548 - acc: 0.9370 - val_loss: 0.1828 - val_acc: 0.9279\n",
      "Epoch 29/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1530 - acc: 0.9370 - val_loss: 0.1812 - val_acc: 0.9266\n",
      "Epoch 30/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1541 - acc: 0.9357 - val_loss: 0.1773 - val_acc: 0.9266\n",
      "Epoch 31/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1529 - acc: 0.9366 - val_loss: 0.1882 - val_acc: 0.9228\n",
      "Epoch 32/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1515 - acc: 0.9357 - val_loss: 0.1778 - val_acc: 0.9228\n",
      "Epoch 33/291\n",
      "2334/2334 [==============================] - 0s 49us/step - loss: 0.1573 - acc: 0.9370 - val_loss: 0.1949 - val_acc: 0.9112\n",
      "Epoch 34/291\n",
      "2334/2334 [==============================] - 0s 53us/step - loss: 0.1583 - acc: 0.9357 - val_loss: 0.1777 - val_acc: 0.9241\n",
      "Epoch 35/291\n",
      "2334/2334 [==============================] - 0s 57us/step - loss: 0.1510 - acc: 0.9366 - val_loss: 0.1801 - val_acc: 0.9292\n",
      "Epoch 36/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1549 - acc: 0.9374 - val_loss: 0.1774 - val_acc: 0.9254\n",
      "Epoch 37/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1537 - acc: 0.9379 - val_loss: 0.1794 - val_acc: 0.9215\n",
      "Epoch 38/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1509 - acc: 0.9366 - val_loss: 0.1945 - val_acc: 0.9176\n",
      "Epoch 39/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1522 - acc: 0.9340 - val_loss: 0.1763 - val_acc: 0.9279\n",
      "Epoch 40/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1497 - acc: 0.9392 - val_loss: 0.1779 - val_acc: 0.9254\n",
      "Epoch 41/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1529 - acc: 0.9374 - val_loss: 0.1995 - val_acc: 0.9138\n",
      "Epoch 42/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1518 - acc: 0.9353 - val_loss: 0.1826 - val_acc: 0.9254\n",
      "Epoch 43/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1499 - acc: 0.9357 - val_loss: 0.1796 - val_acc: 0.9266\n",
      "Epoch 44/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1494 - acc: 0.9387 - val_loss: 0.1923 - val_acc: 0.9151\n",
      "Epoch 45/291\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1565 - acc: 0.9357 - val_loss: 0.1769 - val_acc: 0.9279\n",
      "Epoch 46/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1529 - acc: 0.9374 - val_loss: 0.1802 - val_acc: 0.9254\n",
      "Epoch 47/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1524 - acc: 0.9374 - val_loss: 0.1793 - val_acc: 0.9266\n",
      "Epoch 48/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1528 - acc: 0.9396 - val_loss: 0.1868 - val_acc: 0.9241\n",
      "Epoch 49/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1497 - acc: 0.9409 - val_loss: 0.1754 - val_acc: 0.9344\n",
      "Epoch 50/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1543 - acc: 0.9387 - val_loss: 0.1915 - val_acc: 0.9176\n",
      "Epoch 51/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1505 - acc: 0.9370 - val_loss: 0.1792 - val_acc: 0.9344\n",
      "Epoch 52/291\n",
      "2334/2334 [==============================] - 0s 52us/step - loss: 0.1573 - acc: 0.9319 - val_loss: 0.1809 - val_acc: 0.9279\n",
      "Epoch 53/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1490 - acc: 0.9366 - val_loss: 0.1786 - val_acc: 0.9279\n",
      "Epoch 54/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1564 - acc: 0.9396 - val_loss: 0.1875 - val_acc: 0.9228\n",
      "Epoch 55/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1502 - acc: 0.9387 - val_loss: 0.1744 - val_acc: 0.9305\n",
      "Epoch 56/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1544 - acc: 0.9362 - val_loss: 0.1970 - val_acc: 0.9163\n",
      "Epoch 57/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1556 - acc: 0.9404 - val_loss: 0.1761 - val_acc: 0.9241\n",
      "Epoch 58/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1508 - acc: 0.9383 - val_loss: 0.1858 - val_acc: 0.9228\n",
      "Epoch 59/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1578 - acc: 0.930 - 0s 40us/step - loss: 0.1527 - acc: 0.9336 - val_loss: 0.1835 - val_acc: 0.9228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1493 - acc: 0.9387 - val_loss: 0.1747 - val_acc: 0.9266\n",
      "Epoch 61/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1545 - acc: 0.9383 - val_loss: 0.1737 - val_acc: 0.9279\n",
      "Epoch 62/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1459 - acc: 0.9413 - val_loss: 0.1766 - val_acc: 0.9266\n",
      "Epoch 63/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1493 - acc: 0.9392 - val_loss: 0.1752 - val_acc: 0.9292\n",
      "Epoch 64/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1511 - acc: 0.9340 - val_loss: 0.1909 - val_acc: 0.9189\n",
      "Epoch 65/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1522 - acc: 0.9392 - val_loss: 0.1845 - val_acc: 0.9292\n",
      "Epoch 66/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1520 - acc: 0.9387 - val_loss: 0.1794 - val_acc: 0.9266\n",
      "Epoch 67/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1518 - acc: 0.9370 - val_loss: 0.1766 - val_acc: 0.9279\n",
      "Epoch 68/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1487 - acc: 0.9387 - val_loss: 0.1745 - val_acc: 0.9279\n",
      "Epoch 69/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1546 - acc: 0.9357 - val_loss: 0.1760 - val_acc: 0.9292\n",
      "Epoch 70/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1611 - acc: 0.9340 - val_loss: 0.1770 - val_acc: 0.9279\n",
      "Epoch 71/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1486 - acc: 0.9379 - val_loss: 0.1790 - val_acc: 0.9279\n",
      "Epoch 72/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1462 - acc: 0.9353 - val_loss: 0.1758 - val_acc: 0.9279\n",
      "Epoch 73/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1483 - acc: 0.9396 - val_loss: 0.1743 - val_acc: 0.9254\n",
      "Epoch 74/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1486 - acc: 0.9413 - val_loss: 0.1778 - val_acc: 0.9266\n",
      "Epoch 75/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1486 - acc: 0.9387 - val_loss: 0.1911 - val_acc: 0.9163\n",
      "Epoch 76/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1531 - acc: 0.9366 - val_loss: 0.1757 - val_acc: 0.9266\n",
      "Epoch 77/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1476 - acc: 0.9383 - val_loss: 0.1767 - val_acc: 0.9292\n",
      "Epoch 78/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1462 - acc: 0.9383 - val_loss: 0.1786 - val_acc: 0.9266\n",
      "Epoch 79/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1476 - acc: 0.9379 - val_loss: 0.1740 - val_acc: 0.9292\n",
      "Epoch 80/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1457 - acc: 0.9392 - val_loss: 0.1790 - val_acc: 0.9279\n",
      "Epoch 81/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1482 - acc: 0.9404 - val_loss: 0.1769 - val_acc: 0.9266\n",
      "Epoch 82/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1479 - acc: 0.9383 - val_loss: 0.1741 - val_acc: 0.9266\n",
      "Epoch 83/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1456 - acc: 0.9404 - val_loss: 0.1737 - val_acc: 0.9318\n",
      "Epoch 84/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1449 - acc: 0.9379 - val_loss: 0.1779 - val_acc: 0.9292\n",
      "Epoch 85/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1417 - acc: 0.9422 - val_loss: 0.2262 - val_acc: 0.8996\n",
      "Epoch 86/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1519 - acc: 0.9426 - val_loss: 0.1728 - val_acc: 0.9318\n",
      "Epoch 87/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1436 - acc: 0.9400 - val_loss: 0.1809 - val_acc: 0.9266\n",
      "Epoch 88/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1506 - acc: 0.9383 - val_loss: 0.1721 - val_acc: 0.9318\n",
      "Epoch 89/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1457 - acc: 0.9383 - val_loss: 0.1818 - val_acc: 0.9254\n",
      "Epoch 90/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1464 - acc: 0.9396 - val_loss: 0.1727 - val_acc: 0.9331\n",
      "Epoch 91/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1472 - acc: 0.9379 - val_loss: 0.1737 - val_acc: 0.9279\n",
      "Epoch 92/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1446 - acc: 0.9409 - val_loss: 0.1744 - val_acc: 0.9292\n",
      "Epoch 93/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1453 - acc: 0.9387 - val_loss: 0.1755 - val_acc: 0.9228\n",
      "Epoch 94/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1462 - acc: 0.9374 - val_loss: 0.1988 - val_acc: 0.9151\n",
      "Epoch 95/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1528 - acc: 0.9362 - val_loss: 0.1721 - val_acc: 0.9292\n",
      "Epoch 96/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1502 - acc: 0.9379 - val_loss: 0.1738 - val_acc: 0.9305\n",
      "Epoch 97/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1450 - acc: 0.9404 - val_loss: 0.1717 - val_acc: 0.9318\n",
      "Epoch 98/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1454 - acc: 0.9379 - val_loss: 0.1767 - val_acc: 0.9241\n",
      "Epoch 99/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1437 - acc: 0.9392 - val_loss: 0.1719 - val_acc: 0.9331\n",
      "Epoch 100/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1419 - acc: 0.9400 - val_loss: 0.1825 - val_acc: 0.9241\n",
      "Epoch 101/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1445 - acc: 0.9383 - val_loss: 0.1762 - val_acc: 0.9254\n",
      "Epoch 102/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1474 - acc: 0.9392 - val_loss: 0.1862 - val_acc: 0.9228\n",
      "Epoch 103/291\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1418 - acc: 0.9422 - val_loss: 0.1719 - val_acc: 0.9331\n",
      "Epoch 104/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1424 - acc: 0.9383 - val_loss: 0.1765 - val_acc: 0.9266\n",
      "Epoch 105/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1444 - acc: 0.9417 - val_loss: 0.1752 - val_acc: 0.9292\n",
      "Epoch 106/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1411 - acc: 0.9400 - val_loss: 0.1738 - val_acc: 0.9305\n",
      "Epoch 107/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1410 - acc: 0.9417 - val_loss: 0.1866 - val_acc: 0.9176\n",
      "Epoch 108/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1438 - acc: 0.9383 - val_loss: 0.1775 - val_acc: 0.9331\n",
      "Epoch 109/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1460 - acc: 0.9374 - val_loss: 0.1903 - val_acc: 0.9176\n",
      "Epoch 110/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1434 - acc: 0.9400 - val_loss: 0.1725 - val_acc: 0.9292\n",
      "Epoch 111/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1421 - acc: 0.9392 - val_loss: 0.1791 - val_acc: 0.9279\n",
      "Epoch 112/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1464 - acc: 0.9379 - val_loss: 0.1808 - val_acc: 0.9215\n",
      "Epoch 113/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1491 - acc: 0.9392 - val_loss: 0.1734 - val_acc: 0.9318\n",
      "Epoch 114/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1421 - acc: 0.9404 - val_loss: 0.1786 - val_acc: 0.9266\n",
      "Epoch 115/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1448 - acc: 0.9404 - val_loss: 0.1732 - val_acc: 0.9344\n",
      "Epoch 116/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1427 - acc: 0.9439 - val_loss: 0.1883 - val_acc: 0.9163\n",
      "Epoch 117/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1430 - acc: 0.9422 - val_loss: 0.1733 - val_acc: 0.9305\n",
      "Epoch 118/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1422 - acc: 0.9426 - val_loss: 0.2029 - val_acc: 0.9112\n",
      "Epoch 119/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1451 - acc: 0.9400 - val_loss: 0.1756 - val_acc: 0.9279\n",
      "Epoch 120/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1373 - acc: 0.943 - 0s 39us/step - loss: 0.1424 - acc: 0.9413 - val_loss: 0.1712 - val_acc: 0.9331\n",
      "Epoch 121/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1437 - acc: 0.9430 - val_loss: 0.1755 - val_acc: 0.9305\n",
      "Epoch 122/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1405 - acc: 0.9409 - val_loss: 0.1743 - val_acc: 0.9292\n",
      "Epoch 123/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1403 - acc: 0.9396 - val_loss: 0.1934 - val_acc: 0.9176\n",
      "Epoch 124/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1450 - acc: 0.9409 - val_loss: 0.1721 - val_acc: 0.9305\n",
      "Epoch 125/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1445 - acc: 0.946 - 0s 43us/step - loss: 0.1435 - acc: 0.9434 - val_loss: 0.1725 - val_acc: 0.9292\n",
      "Epoch 126/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1413 - acc: 0.9439 - val_loss: 0.1721 - val_acc: 0.9292\n",
      "Epoch 127/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1409 - acc: 0.9430 - val_loss: 0.1712 - val_acc: 0.9344\n",
      "Epoch 128/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1434 - acc: 0.9417 - val_loss: 0.1715 - val_acc: 0.9305\n",
      "Epoch 129/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1449 - acc: 0.9387 - val_loss: 0.1739 - val_acc: 0.9318\n",
      "Epoch 130/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1403 - acc: 0.9400 - val_loss: 0.1720 - val_acc: 0.9344\n",
      "Epoch 131/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1400 - acc: 0.9404 - val_loss: 0.1711 - val_acc: 0.9331\n",
      "Epoch 132/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1404 - acc: 0.9422 - val_loss: 0.1705 - val_acc: 0.9305\n",
      "Epoch 133/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1410 - acc: 0.9387 - val_loss: 0.1879 - val_acc: 0.9202\n",
      "Epoch 134/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1425 - acc: 0.9400 - val_loss: 0.1722 - val_acc: 0.9331\n",
      "Epoch 135/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1393 - acc: 0.9422 - val_loss: 0.1721 - val_acc: 0.9331\n",
      "Epoch 136/291\n",
      "2334/2334 [==============================] - 0s 65us/step - loss: 0.1404 - acc: 0.9434 - val_loss: 0.1729 - val_acc: 0.9318\n",
      "Epoch 137/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1475 - acc: 0.9413 - val_loss: 0.1801 - val_acc: 0.9215\n",
      "Epoch 138/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1418 - acc: 0.9404 - val_loss: 0.1724 - val_acc: 0.9305\n",
      "Epoch 139/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1391 - acc: 0.9396 - val_loss: 0.1697 - val_acc: 0.9344\n",
      "Epoch 140/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1379 - acc: 0.9426 - val_loss: 0.1715 - val_acc: 0.9318\n",
      "Epoch 141/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1388 - acc: 0.9439 - val_loss: 0.1701 - val_acc: 0.9344\n",
      "Epoch 142/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1403 - acc: 0.9413 - val_loss: 0.1720 - val_acc: 0.9305\n",
      "Epoch 143/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1381 - acc: 0.9439 - val_loss: 0.1716 - val_acc: 0.9318\n",
      "Epoch 144/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1387 - acc: 0.9422 - val_loss: 0.1833 - val_acc: 0.9228\n",
      "Epoch 145/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1398 - acc: 0.9439 - val_loss: 0.1840 - val_acc: 0.9241\n",
      "Epoch 146/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1393 - acc: 0.9409 - val_loss: 0.1712 - val_acc: 0.9305\n",
      "Epoch 147/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1399 - acc: 0.9434 - val_loss: 0.1706 - val_acc: 0.9331\n",
      "Epoch 148/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1409 - acc: 0.9409 - val_loss: 0.1735 - val_acc: 0.9305\n",
      "Epoch 149/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1378 - acc: 0.9417 - val_loss: 0.1914 - val_acc: 0.9189\n",
      "Epoch 150/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1410 - acc: 0.9392 - val_loss: 0.1947 - val_acc: 0.9163\n",
      "Epoch 151/291\n",
      "2334/2334 [==============================] - 0s 54us/step - loss: 0.1422 - acc: 0.9434 - val_loss: 0.1716 - val_acc: 0.9344\n",
      "Epoch 152/291\n",
      "2334/2334 [==============================] - 0s 56us/step - loss: 0.1386 - acc: 0.9447 - val_loss: 0.1811 - val_acc: 0.9241\n",
      "Epoch 153/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1377 - acc: 0.9452 - val_loss: 0.1715 - val_acc: 0.9331\n",
      "Epoch 154/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1418 - acc: 0.9409 - val_loss: 0.1942 - val_acc: 0.9163\n",
      "Epoch 155/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1410 - acc: 0.9413 - val_loss: 0.1798 - val_acc: 0.9318\n",
      "Epoch 156/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1433 - acc: 0.9404 - val_loss: 0.1773 - val_acc: 0.9279\n",
      "Epoch 157/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1394 - acc: 0.9452 - val_loss: 0.1697 - val_acc: 0.9331\n",
      "Epoch 158/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1397 - acc: 0.9426 - val_loss: 0.1828 - val_acc: 0.9228\n",
      "Epoch 159/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1508 - acc: 0.9413 - val_loss: 0.1750 - val_acc: 0.9292\n",
      "Epoch 160/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1421 - acc: 0.9456 - val_loss: 0.1830 - val_acc: 0.9202\n",
      "Epoch 161/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1386 - acc: 0.9422 - val_loss: 0.1701 - val_acc: 0.9318\n",
      "Epoch 162/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1388 - acc: 0.9430 - val_loss: 0.1828 - val_acc: 0.9228\n",
      "Epoch 163/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1362 - acc: 0.9430 - val_loss: 0.1718 - val_acc: 0.9331\n",
      "Epoch 164/291\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1382 - acc: 0.9426 - val_loss: 0.1946 - val_acc: 0.9163\n",
      "Epoch 165/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1372 - acc: 0.9456 - val_loss: 0.1787 - val_acc: 0.9279\n",
      "Epoch 166/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1378 - acc: 0.9439 - val_loss: 0.1704 - val_acc: 0.9344\n",
      "Epoch 167/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1427 - acc: 0.9422 - val_loss: 0.1966 - val_acc: 0.9151\n",
      "Epoch 168/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1403 - acc: 0.9434 - val_loss: 0.1802 - val_acc: 0.9266\n",
      "Epoch 169/291\n",
      "2334/2334 [==============================] - ETA: 0s - loss: 0.1312 - acc: 0.945 - 0s 41us/step - loss: 0.1375 - acc: 0.9426 - val_loss: 0.1700 - val_acc: 0.9331\n",
      "Epoch 170/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1359 - acc: 0.9486 - val_loss: 0.1694 - val_acc: 0.9344\n",
      "Epoch 171/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1360 - acc: 0.9426 - val_loss: 0.1743 - val_acc: 0.9331\n",
      "Epoch 172/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1363 - acc: 0.9434 - val_loss: 0.1764 - val_acc: 0.9305\n",
      "Epoch 173/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1364 - acc: 0.9456 - val_loss: 0.1778 - val_acc: 0.9292\n",
      "Epoch 174/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1407 - acc: 0.9409 - val_loss: 0.1717 - val_acc: 0.9318\n",
      "Epoch 175/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1452 - acc: 0.9426 - val_loss: 0.1779 - val_acc: 0.9305\n",
      "Epoch 176/291\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1366 - acc: 0.9456 - val_loss: 0.1732 - val_acc: 0.9318\n",
      "Epoch 177/291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1361 - acc: 0.9447 - val_loss: 0.1703 - val_acc: 0.9305\n",
      "Epoch 178/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1352 - acc: 0.9439 - val_loss: 0.1745 - val_acc: 0.9318\n",
      "Epoch 179/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1361 - acc: 0.9456 - val_loss: 0.1725 - val_acc: 0.9266\n",
      "Epoch 180/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1358 - acc: 0.9473 - val_loss: 0.1728 - val_acc: 0.9331\n",
      "Epoch 181/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1371 - acc: 0.9443 - val_loss: 0.1707 - val_acc: 0.9318\n",
      "Epoch 182/291\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1369 - acc: 0.9409 - val_loss: 0.1937 - val_acc: 0.9189\n",
      "Epoch 183/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1543 - acc: 0.9387 - val_loss: 0.1723 - val_acc: 0.9305\n",
      "Epoch 184/291\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1414 - acc: 0.9439 - val_loss: 0.1736 - val_acc: 0.9305\n",
      "Epoch 185/291\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1420 - acc: 0.9439 - val_loss: 0.1761 - val_acc: 0.9356\n",
      "Epoch 186/291\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1376 - acc: 0.9439 - val_loss: 0.1773 - val_acc: 0.9279\n",
      "Epoch 187/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1342 - acc: 0.9464 - val_loss: 0.1875 - val_acc: 0.9215\n",
      "Epoch 188/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1354 - acc: 0.9439 - val_loss: 0.1708 - val_acc: 0.9331\n",
      "Epoch 189/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1340 - acc: 0.9430 - val_loss: 0.1720 - val_acc: 0.9318\n",
      "Epoch 190/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1342 - acc: 0.9439 - val_loss: 0.1799 - val_acc: 0.9266\n",
      "Epoch 191/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1340 - acc: 0.9464 - val_loss: 0.1691 - val_acc: 0.9305\n",
      "Epoch 192/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1341 - acc: 0.9460 - val_loss: 0.1709 - val_acc: 0.9318\n",
      "Epoch 193/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1384 - acc: 0.9426 - val_loss: 0.1711 - val_acc: 0.9318\n",
      "Epoch 194/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1341 - acc: 0.9439 - val_loss: 0.1702 - val_acc: 0.9318\n",
      "Epoch 195/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1341 - acc: 0.9439 - val_loss: 0.1740 - val_acc: 0.9331\n",
      "Epoch 196/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1364 - acc: 0.9443 - val_loss: 0.1743 - val_acc: 0.9292\n",
      "Epoch 197/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1355 - acc: 0.9469 - val_loss: 0.1702 - val_acc: 0.9331\n",
      "Epoch 198/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1403 - acc: 0.9430 - val_loss: 0.1695 - val_acc: 0.9318\n",
      "Epoch 199/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1352 - acc: 0.9460 - val_loss: 0.1715 - val_acc: 0.9318\n",
      "Epoch 200/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1325 - acc: 0.9464 - val_loss: 0.1695 - val_acc: 0.9331\n",
      "Epoch 201/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1350 - acc: 0.9447 - val_loss: 0.1944 - val_acc: 0.9189\n",
      "Epoch 202/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1403 - acc: 0.9447 - val_loss: 0.1795 - val_acc: 0.9331\n",
      "Epoch 203/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1430 - acc: 0.9404 - val_loss: 0.1768 - val_acc: 0.9279\n",
      "Epoch 204/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1350 - acc: 0.9452 - val_loss: 0.1692 - val_acc: 0.9344\n",
      "Epoch 205/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1361 - acc: 0.9439 - val_loss: 0.1824 - val_acc: 0.9241\n",
      "Epoch 206/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1369 - acc: 0.9443 - val_loss: 0.1741 - val_acc: 0.9305\n",
      "Epoch 207/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1332 - acc: 0.9460 - val_loss: 0.1711 - val_acc: 0.9318\n",
      "Epoch 208/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1321 - acc: 0.9469 - val_loss: 0.1684 - val_acc: 0.9331\n",
      "Epoch 209/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1374 - acc: 0.9409 - val_loss: 0.1684 - val_acc: 0.9344\n",
      "Epoch 210/291\n",
      "2334/2334 [==============================] - 0s 63us/step - loss: 0.1427 - acc: 0.9422 - val_loss: 0.1888 - val_acc: 0.9189\n",
      "Epoch 211/291\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1351 - acc: 0.9469 - val_loss: 0.1712 - val_acc: 0.9305\n",
      "Epoch 212/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1337 - acc: 0.9456 - val_loss: 0.1729 - val_acc: 0.9331\n",
      "Epoch 213/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1337 - acc: 0.9490 - val_loss: 0.1747 - val_acc: 0.9305\n",
      "Epoch 214/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1324 - acc: 0.9456 - val_loss: 0.1723 - val_acc: 0.9318\n",
      "Epoch 215/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1335 - acc: 0.9477 - val_loss: 0.1742 - val_acc: 0.9305\n",
      "Epoch 216/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1324 - acc: 0.9473 - val_loss: 0.1681 - val_acc: 0.9305\n",
      "Epoch 217/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1337 - acc: 0.9464 - val_loss: 0.1850 - val_acc: 0.9254\n",
      "Epoch 218/291\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1335 - acc: 0.9456 - val_loss: 0.1910 - val_acc: 0.9151\n",
      "Epoch 219/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1353 - acc: 0.9443 - val_loss: 0.1839 - val_acc: 0.9318\n",
      "Epoch 220/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1392 - acc: 0.9413 - val_loss: 0.1777 - val_acc: 0.9279\n",
      "Epoch 221/291\n",
      "2334/2334 [==============================] - 0s 49us/step - loss: 0.1349 - acc: 0.9473 - val_loss: 0.1772 - val_acc: 0.9331\n",
      "Epoch 222/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1366 - acc: 0.9443 - val_loss: 0.1718 - val_acc: 0.9292\n",
      "Epoch 223/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1302 - acc: 0.9473 - val_loss: 0.1906 - val_acc: 0.9215\n",
      "Epoch 224/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1396 - acc: 0.9447 - val_loss: 0.1713 - val_acc: 0.9318\n",
      "Epoch 225/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1323 - acc: 0.9456 - val_loss: 0.1705 - val_acc: 0.9318\n",
      "Epoch 226/291\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1332 - acc: 0.9456 - val_loss: 0.1814 - val_acc: 0.9254\n",
      "Epoch 227/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1328 - acc: 0.9456 - val_loss: 0.1759 - val_acc: 0.9331\n",
      "Epoch 228/291\n",
      "2334/2334 [==============================] - 0s 50us/step - loss: 0.1372 - acc: 0.9494 - val_loss: 0.1709 - val_acc: 0.9344\n",
      "Epoch 229/291\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1335 - acc: 0.9490 - val_loss: 0.1691 - val_acc: 0.9344\n",
      "Epoch 230/291\n",
      "2334/2334 [==============================] - 0s 51us/step - loss: 0.1323 - acc: 0.9452 - val_loss: 0.1724 - val_acc: 0.9331\n",
      "Epoch 231/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1312 - acc: 0.9469 - val_loss: 0.1706 - val_acc: 0.9318\n",
      "Epoch 232/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1367 - acc: 0.9447 - val_loss: 0.1690 - val_acc: 0.9344\n",
      "Epoch 233/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1311 - acc: 0.9469 - val_loss: 0.1685 - val_acc: 0.9344\n",
      "Epoch 234/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1327 - acc: 0.9473 - val_loss: 0.1770 - val_acc: 0.9279\n",
      "Epoch 235/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1329 - acc: 0.9464 - val_loss: 0.1685 - val_acc: 0.9318\n",
      "Epoch 236/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1329 - acc: 0.9473 - val_loss: 0.1683 - val_acc: 0.9318\n",
      "Epoch 237/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1373 - acc: 0.9452 - val_loss: 0.1741 - val_acc: 0.9292\n",
      "Epoch 238/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1365 - acc: 0.9464 - val_loss: 0.1768 - val_acc: 0.9292\n",
      "Epoch 239/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1382 - acc: 0.9434 - val_loss: 0.1682 - val_acc: 0.9331\n",
      "Epoch 240/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1479 - acc: 0.9417 - val_loss: 0.1720 - val_acc: 0.9318\n",
      "Epoch 241/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1393 - acc: 0.9430 - val_loss: 0.1726 - val_acc: 0.9305\n",
      "Epoch 242/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1342 - acc: 0.9482 - val_loss: 0.1761 - val_acc: 0.9279\n",
      "Epoch 243/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1323 - acc: 0.9482 - val_loss: 0.1708 - val_acc: 0.9331\n",
      "Epoch 244/291\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1316 - acc: 0.9486 - val_loss: 0.1709 - val_acc: 0.9369\n",
      "Epoch 245/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1323 - acc: 0.9482 - val_loss: 0.1701 - val_acc: 0.9331\n",
      "Epoch 246/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1308 - acc: 0.9482 - val_loss: 0.1712 - val_acc: 0.9331\n",
      "Epoch 247/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1327 - acc: 0.9473 - val_loss: 0.1687 - val_acc: 0.9318\n",
      "Epoch 248/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1313 - acc: 0.9473 - val_loss: 0.1722 - val_acc: 0.9318\n",
      "Epoch 249/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1363 - acc: 0.9430 - val_loss: 0.1719 - val_acc: 0.9305\n",
      "Epoch 250/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1359 - acc: 0.9439 - val_loss: 0.1727 - val_acc: 0.9318\n",
      "Epoch 251/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1402 - acc: 0.9404 - val_loss: 0.1797 - val_acc: 0.9331\n",
      "Epoch 252/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1372 - acc: 0.9452 - val_loss: 0.1732 - val_acc: 0.9331\n",
      "Epoch 253/291\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1298 - acc: 0.9482 - val_loss: 0.1712 - val_acc: 0.9318\n",
      "Epoch 254/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1311 - acc: 0.9456 - val_loss: 0.1701 - val_acc: 0.9331\n",
      "Epoch 255/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1324 - acc: 0.9499 - val_loss: 0.1763 - val_acc: 0.9292\n",
      "Epoch 256/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1302 - acc: 0.9469 - val_loss: 0.1690 - val_acc: 0.9331\n",
      "Epoch 257/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1327 - acc: 0.9456 - val_loss: 0.1700 - val_acc: 0.9331\n",
      "Epoch 258/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1305 - acc: 0.9499 - val_loss: 0.1717 - val_acc: 0.9318\n",
      "Epoch 259/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1312 - acc: 0.9473 - val_loss: 0.1694 - val_acc: 0.9356\n",
      "Epoch 260/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1361 - acc: 0.9430 - val_loss: 0.1751 - val_acc: 0.9292\n",
      "Epoch 261/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1295 - acc: 0.9494 - val_loss: 0.1690 - val_acc: 0.9331\n",
      "Epoch 262/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1310 - acc: 0.9464 - val_loss: 0.1948 - val_acc: 0.9163\n",
      "Epoch 263/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1346 - acc: 0.9452 - val_loss: 0.1691 - val_acc: 0.9305\n",
      "Epoch 264/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1343 - acc: 0.9477 - val_loss: 0.1706 - val_acc: 0.9318\n",
      "Epoch 265/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1356 - acc: 0.9404 - val_loss: 0.1694 - val_acc: 0.9331\n",
      "Epoch 266/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1316 - acc: 0.9486 - val_loss: 0.1694 - val_acc: 0.9331\n",
      "Epoch 267/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1295 - acc: 0.9486 - val_loss: 0.1766 - val_acc: 0.9292\n",
      "Epoch 268/291\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1428 - acc: 0.9404 - val_loss: 0.2170 - val_acc: 0.9112\n",
      "Epoch 269/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1448 - acc: 0.9439 - val_loss: 0.1911 - val_acc: 0.9331\n",
      "Epoch 270/291\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1409 - acc: 0.9447 - val_loss: 0.2002 - val_acc: 0.9151\n",
      "Epoch 271/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1320 - acc: 0.9473 - val_loss: 0.1726 - val_acc: 0.9318\n",
      "Epoch 272/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1319 - acc: 0.9447 - val_loss: 0.1700 - val_acc: 0.9318\n",
      "Epoch 273/291\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1288 - acc: 0.9494 - val_loss: 0.1707 - val_acc: 0.9318\n",
      "Epoch 274/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1317 - acc: 0.9456 - val_loss: 0.1822 - val_acc: 0.9241\n",
      "Epoch 275/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1340 - acc: 0.9464 - val_loss: 0.1755 - val_acc: 0.9305\n",
      "Epoch 276/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1349 - acc: 0.9486 - val_loss: 0.1757 - val_acc: 0.9292\n",
      "Epoch 277/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1325 - acc: 0.9482 - val_loss: 0.1806 - val_acc: 0.9266\n",
      "Epoch 278/291\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1317 - acc: 0.9434 - val_loss: 0.1704 - val_acc: 0.9344\n",
      "Epoch 279/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1308 - acc: 0.9447 - val_loss: 0.1705 - val_acc: 0.9331\n",
      "Epoch 280/291\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1297 - acc: 0.9477 - val_loss: 0.1775 - val_acc: 0.9279\n",
      "Epoch 281/291\n",
      "2334/2334 [==============================] - 0s 49us/step - loss: 0.1297 - acc: 0.9486 - val_loss: 0.1688 - val_acc: 0.9331\n",
      "Epoch 282/291\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1311 - acc: 0.9469 - val_loss: 0.1800 - val_acc: 0.9266\n",
      "Epoch 283/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1303 - acc: 0.9477 - val_loss: 0.1684 - val_acc: 0.9318\n",
      "Epoch 284/291\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1311 - acc: 0.9447 - val_loss: 0.1725 - val_acc: 0.9279\n",
      "Epoch 285/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1325 - acc: 0.9443 - val_loss: 0.1763 - val_acc: 0.9279\n",
      "Epoch 286/291\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1323 - acc: 0.9460 - val_loss: 0.1715 - val_acc: 0.9318\n",
      "Epoch 287/291\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1330 - acc: 0.9464 - val_loss: 0.1718 - val_acc: 0.9318\n",
      "Epoch 288/291\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1313 - acc: 0.9456 - val_loss: 0.1748 - val_acc: 0.9292\n",
      "Epoch 289/291\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1306 - acc: 0.9482 - val_loss: 0.1704 - val_acc: 0.9344\n",
      "Epoch 290/291\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1270 - acc: 0.9507 - val_loss: 0.1766 - val_acc: 0.9279\n",
      "Epoch 291/291\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1306 - acc: 0.9456 - val_loss: 0.1716 - val_acc: 0.9305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f5f1fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtr, ytr, epochs=291, batch_size=100, validation_data=(xts, yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing samples= 0.930501930502\n"
     ]
    }
   ],
   "source": [
    "score, acc1 = model.evaluate(xts, yts, verbose=0)\n",
    "print(\"Accuracy on testing samples=\", acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with the best feature combination from subset regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns np array made of columns named i feature list\n",
    "def colum(df, feature):\n",
    "    return np.column_stack([ np.array(df[f]) for f in feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ('population', 'population_change', 'Black', 'Hispanic', 'Edu_bachelors', 'income', 'Poverty', 'Density')\n",
    "xtr_best = colum(df_train, col)\n",
    "xts_best = colum(df_test, col)\n",
    "\n",
    "xtr_best = (xtr_best - np.mean(xtr_best, axis=0))/np.std(xtr_best, axis=0)\n",
    "xts_best = (xts_best - np.mean(xts_best, axis=0))/np.std(xts_best, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 1000)              9000      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 11,002\n",
      "Trainable params: 11,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "nh=1000             #number of hidden units\n",
    "N = xtr_best.shape[1]  #number of features, aka number of input neurons\n",
    "M = 2             #number of output neurons. \n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(N,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(M, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2334 samples, validate on 777 samples\n",
      "Epoch 1/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1300 - acc: 0.9460 - val_loss: 0.1904 - val_acc: 0.9318\n",
      "Epoch 2/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1315 - acc: 0.9464 - val_loss: 0.1806 - val_acc: 0.9369\n",
      "Epoch 3/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1238 - acc: 0.9524 - val_loss: 0.1775 - val_acc: 0.9318\n",
      "Epoch 4/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1228 - acc: 0.9486 - val_loss: 0.1787 - val_acc: 0.9344\n",
      "Epoch 5/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1243 - acc: 0.9512 - val_loss: 0.1873 - val_acc: 0.9292\n",
      "Epoch 6/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1231 - acc: 0.9490 - val_loss: 0.1793 - val_acc: 0.9318\n",
      "Epoch 7/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1210 - acc: 0.9507 - val_loss: 0.1769 - val_acc: 0.9344\n",
      "Epoch 8/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1227 - acc: 0.9499 - val_loss: 0.1836 - val_acc: 0.9305\n",
      "Epoch 9/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1229 - acc: 0.9494 - val_loss: 0.1796 - val_acc: 0.9331\n",
      "Epoch 10/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1214 - acc: 0.9499 - val_loss: 0.1774 - val_acc: 0.9331\n",
      "Epoch 11/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1229 - acc: 0.9512 - val_loss: 0.1879 - val_acc: 0.9279\n",
      "Epoch 12/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1245 - acc: 0.9494 - val_loss: 0.1759 - val_acc: 0.9318\n",
      "Epoch 13/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1249 - acc: 0.9469 - val_loss: 0.1770 - val_acc: 0.9292\n",
      "Epoch 14/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1231 - acc: 0.9537 - val_loss: 0.1767 - val_acc: 0.9369\n",
      "Epoch 15/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1237 - acc: 0.9512 - val_loss: 0.1776 - val_acc: 0.9356\n",
      "Epoch 16/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1204 - acc: 0.9499 - val_loss: 0.1774 - val_acc: 0.9318\n",
      "Epoch 17/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1252 - acc: 0.9503 - val_loss: 0.1776 - val_acc: 0.9344\n",
      "Epoch 18/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1211 - acc: 0.9516 - val_loss: 0.1861 - val_acc: 0.9305\n",
      "Epoch 19/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1227 - acc: 0.9494 - val_loss: 0.1830 - val_acc: 0.9318\n",
      "Epoch 20/200\n",
      "2334/2334 [==============================] - 0s 59us/step - loss: 0.1220 - acc: 0.9516 - val_loss: 0.1771 - val_acc: 0.9356\n",
      "Epoch 21/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1218 - acc: 0.9524 - val_loss: 0.1769 - val_acc: 0.9344\n",
      "Epoch 22/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1235 - acc: 0.9503 - val_loss: 0.1768 - val_acc: 0.9344\n",
      "Epoch 23/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1219 - acc: 0.9499 - val_loss: 0.1767 - val_acc: 0.9356\n",
      "Epoch 24/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1225 - acc: 0.9537 - val_loss: 0.1867 - val_acc: 0.9279\n",
      "Epoch 25/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1222 - acc: 0.9512 - val_loss: 0.1762 - val_acc: 0.9356\n",
      "Epoch 26/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1227 - acc: 0.9490 - val_loss: 0.1810 - val_acc: 0.9344\n",
      "Epoch 27/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1251 - acc: 0.9482 - val_loss: 0.1829 - val_acc: 0.9331\n",
      "Epoch 28/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1220 - acc: 0.9507 - val_loss: 0.1976 - val_acc: 0.9189\n",
      "Epoch 29/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1250 - acc: 0.9490 - val_loss: 0.1795 - val_acc: 0.9344\n",
      "Epoch 30/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1219 - acc: 0.9529 - val_loss: 0.1835 - val_acc: 0.9331\n",
      "Epoch 31/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1256 - acc: 0.9499 - val_loss: 0.1781 - val_acc: 0.9356\n",
      "Epoch 32/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1256 - acc: 0.9477 - val_loss: 0.1807 - val_acc: 0.9331\n",
      "Epoch 33/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1216 - acc: 0.9503 - val_loss: 0.1780 - val_acc: 0.9356\n",
      "Epoch 34/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1224 - acc: 0.9499 - val_loss: 0.1798 - val_acc: 0.9331\n",
      "Epoch 35/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.1297 - acc: 0.9499 - val_loss: 0.1796 - val_acc: 0.9344\n",
      "Epoch 36/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1216 - acc: 0.9482 - val_loss: 0.1812 - val_acc: 0.9318\n",
      "Epoch 37/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1218 - acc: 0.9516 - val_loss: 0.1825 - val_acc: 0.9331\n",
      "Epoch 38/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1237 - acc: 0.9516 - val_loss: 0.1804 - val_acc: 0.9382\n",
      "Epoch 39/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1285 - acc: 0.9516 - val_loss: 0.1985 - val_acc: 0.9228\n",
      "Epoch 40/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1271 - acc: 0.9503 - val_loss: 0.1779 - val_acc: 0.9331\n",
      "Epoch 41/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1202 - acc: 0.9512 - val_loss: 0.1798 - val_acc: 0.9318\n",
      "Epoch 42/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1221 - acc: 0.9503 - val_loss: 0.1896 - val_acc: 0.9254\n",
      "Epoch 43/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1219 - acc: 0.9503 - val_loss: 0.1802 - val_acc: 0.9356\n",
      "Epoch 44/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1221 - acc: 0.9516 - val_loss: 0.1891 - val_acc: 0.9292\n",
      "Epoch 45/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1296 - acc: 0.9447 - val_loss: 0.1901 - val_acc: 0.9254\n",
      "Epoch 46/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1246 - acc: 0.9503 - val_loss: 0.1772 - val_acc: 0.9344\n",
      "Epoch 47/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1228 - acc: 0.9486 - val_loss: 0.1816 - val_acc: 0.9331\n",
      "Epoch 48/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1196 - acc: 0.9494 - val_loss: 0.1822 - val_acc: 0.9344\n",
      "Epoch 49/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1239 - acc: 0.9524 - val_loss: 0.1755 - val_acc: 0.9292\n",
      "Epoch 50/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1276 - acc: 0.9460 - val_loss: 0.1922 - val_acc: 0.9266\n",
      "Epoch 51/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1226 - acc: 0.9512 - val_loss: 0.1810 - val_acc: 0.9318\n",
      "Epoch 52/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1263 - acc: 0.9507 - val_loss: 0.1919 - val_acc: 0.9279\n",
      "Epoch 53/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1406 - acc: 0.9452 - val_loss: 0.1792 - val_acc: 0.9356\n",
      "Epoch 54/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1242 - acc: 0.9490 - val_loss: 0.1806 - val_acc: 0.9279\n",
      "Epoch 55/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1247 - acc: 0.9469 - val_loss: 0.1784 - val_acc: 0.9331\n",
      "Epoch 56/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1207 - acc: 0.9503 - val_loss: 0.1796 - val_acc: 0.9369\n",
      "Epoch 57/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1176 - acc: 0.9529 - val_loss: 0.1852 - val_acc: 0.9305\n",
      "Epoch 58/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1279 - acc: 0.9473 - val_loss: 0.1797 - val_acc: 0.9344\n",
      "Epoch 59/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1188 - acc: 0.9499 - val_loss: 0.1860 - val_acc: 0.9305\n",
      "Epoch 60/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1204 - acc: 0.9516 - val_loss: 0.1797 - val_acc: 0.9279\n",
      "Epoch 61/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1230 - acc: 0.9482 - val_loss: 0.1877 - val_acc: 0.9279\n",
      "Epoch 62/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1210 - acc: 0.9507 - val_loss: 0.1811 - val_acc: 0.9331\n",
      "Epoch 63/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1274 - acc: 0.9477 - val_loss: 0.1958 - val_acc: 0.9266\n",
      "Epoch 64/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1215 - acc: 0.9482 - val_loss: 0.1836 - val_acc: 0.9344\n",
      "Epoch 65/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1210 - acc: 0.9494 - val_loss: 0.1773 - val_acc: 0.9356\n",
      "Epoch 66/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1218 - acc: 0.9524 - val_loss: 0.1842 - val_acc: 0.9344\n",
      "Epoch 67/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1241 - acc: 0.9503 - val_loss: 0.1803 - val_acc: 0.9344\n",
      "Epoch 68/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1268 - acc: 0.9477 - val_loss: 0.1816 - val_acc: 0.9331\n",
      "Epoch 69/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1246 - acc: 0.9456 - val_loss: 0.1940 - val_acc: 0.9292\n",
      "Epoch 70/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1248 - acc: 0.9490 - val_loss: 0.1797 - val_acc: 0.9279\n",
      "Epoch 71/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1205 - acc: 0.9512 - val_loss: 0.1790 - val_acc: 0.9356\n",
      "Epoch 72/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1210 - acc: 0.9529 - val_loss: 0.1790 - val_acc: 0.9344\n",
      "Epoch 73/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.1212 - acc: 0.9490 - val_loss: 0.1833 - val_acc: 0.9292\n",
      "Epoch 74/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1223 - acc: 0.9499 - val_loss: 0.1770 - val_acc: 0.9344\n",
      "Epoch 75/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1205 - acc: 0.9520 - val_loss: 0.1775 - val_acc: 0.9356\n",
      "Epoch 76/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1198 - acc: 0.9516 - val_loss: 0.1829 - val_acc: 0.9344\n",
      "Epoch 77/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1190 - acc: 0.9537 - val_loss: 0.1817 - val_acc: 0.9331\n",
      "Epoch 78/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1244 - acc: 0.9486 - val_loss: 0.1806 - val_acc: 0.9318\n",
      "Epoch 79/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1217 - acc: 0.9503 - val_loss: 0.1960 - val_acc: 0.9254\n",
      "Epoch 80/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.1235 - acc: 0.9503 - val_loss: 0.1928 - val_acc: 0.9266\n",
      "Epoch 81/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1204 - acc: 0.9524 - val_loss: 0.1820 - val_acc: 0.9356\n",
      "Epoch 82/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1188 - acc: 0.9516 - val_loss: 0.1794 - val_acc: 0.9344\n",
      "Epoch 83/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1287 - acc: 0.9490 - val_loss: 0.1821 - val_acc: 0.9356\n",
      "Epoch 84/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1203 - acc: 0.9537 - val_loss: 0.1988 - val_acc: 0.9241\n",
      "Epoch 85/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1294 - acc: 0.9452 - val_loss: 0.1813 - val_acc: 0.9292\n",
      "Epoch 86/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1223 - acc: 0.9499 - val_loss: 0.1843 - val_acc: 0.9305\n",
      "Epoch 87/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1217 - acc: 0.9516 - val_loss: 0.1788 - val_acc: 0.9356\n",
      "Epoch 88/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1219 - acc: 0.9494 - val_loss: 0.1787 - val_acc: 0.9305\n",
      "Epoch 89/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1187 - acc: 0.9503 - val_loss: 0.1839 - val_acc: 0.9279\n",
      "Epoch 90/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1223 - acc: 0.9499 - val_loss: 0.1793 - val_acc: 0.9356\n",
      "Epoch 91/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1248 - acc: 0.9460 - val_loss: 0.1931 - val_acc: 0.9279\n",
      "Epoch 92/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1223 - acc: 0.9499 - val_loss: 0.1801 - val_acc: 0.9356\n",
      "Epoch 93/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1187 - acc: 0.9503 - val_loss: 0.1838 - val_acc: 0.9305\n",
      "Epoch 94/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1265 - acc: 0.9464 - val_loss: 0.1882 - val_acc: 0.9305\n",
      "Epoch 95/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1201 - acc: 0.9516 - val_loss: 0.1799 - val_acc: 0.9369\n",
      "Epoch 96/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1182 - acc: 0.9516 - val_loss: 0.1787 - val_acc: 0.9369\n",
      "Epoch 97/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1183 - acc: 0.9542 - val_loss: 0.1803 - val_acc: 0.9331\n",
      "Epoch 98/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1200 - acc: 0.9516 - val_loss: 0.1842 - val_acc: 0.9318\n",
      "Epoch 99/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1187 - acc: 0.9516 - val_loss: 0.1832 - val_acc: 0.9318\n",
      "Epoch 100/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1206 - acc: 0.9494 - val_loss: 0.1800 - val_acc: 0.9356\n",
      "Epoch 101/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1212 - acc: 0.9512 - val_loss: 0.1819 - val_acc: 0.9318\n",
      "Epoch 102/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1266 - acc: 0.9464 - val_loss: 0.1788 - val_acc: 0.9344\n",
      "Epoch 103/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1194 - acc: 0.9512 - val_loss: 0.1792 - val_acc: 0.9344\n",
      "Epoch 104/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1174 - acc: 0.9529 - val_loss: 0.1828 - val_acc: 0.9305\n",
      "Epoch 105/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1332 - acc: 0.9439 - val_loss: 0.1824 - val_acc: 0.9318\n",
      "Epoch 106/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1209 - acc: 0.9512 - val_loss: 0.1824 - val_acc: 0.9318\n",
      "Epoch 107/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1194 - acc: 0.9529 - val_loss: 0.1951 - val_acc: 0.9254\n",
      "Epoch 108/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1194 - acc: 0.9507 - val_loss: 0.1800 - val_acc: 0.9369\n",
      "Epoch 109/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1183 - acc: 0.9529 - val_loss: 0.1779 - val_acc: 0.9356\n",
      "Epoch 110/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1195 - acc: 0.9494 - val_loss: 0.1798 - val_acc: 0.9356\n",
      "Epoch 111/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1189 - acc: 0.9516 - val_loss: 0.1802 - val_acc: 0.9305\n",
      "Epoch 112/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1205 - acc: 0.9507 - val_loss: 0.1820 - val_acc: 0.9356\n",
      "Epoch 113/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1175 - acc: 0.9529 - val_loss: 0.1840 - val_acc: 0.9344\n",
      "Epoch 114/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1197 - acc: 0.9516 - val_loss: 0.1805 - val_acc: 0.9356\n",
      "Epoch 115/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1205 - acc: 0.9520 - val_loss: 0.1961 - val_acc: 0.9228\n",
      "Epoch 116/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1211 - acc: 0.9512 - val_loss: 0.1858 - val_acc: 0.9318\n",
      "Epoch 117/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1205 - acc: 0.9516 - val_loss: 0.1835 - val_acc: 0.9356\n",
      "Epoch 118/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1178 - acc: 0.9529 - val_loss: 0.1853 - val_acc: 0.9318\n",
      "Epoch 119/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1207 - acc: 0.9494 - val_loss: 0.1830 - val_acc: 0.9344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1200 - acc: 0.9516 - val_loss: 0.1795 - val_acc: 0.9344\n",
      "Epoch 121/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1202 - acc: 0.9507 - val_loss: 0.1831 - val_acc: 0.9344\n",
      "Epoch 122/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1197 - acc: 0.9499 - val_loss: 0.1832 - val_acc: 0.9331\n",
      "Epoch 123/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1184 - acc: 0.9529 - val_loss: 0.1814 - val_acc: 0.9344\n",
      "Epoch 124/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1210 - acc: 0.9477 - val_loss: 0.1921 - val_acc: 0.9266\n",
      "Epoch 125/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1244 - acc: 0.9499 - val_loss: 0.1792 - val_acc: 0.9356\n",
      "Epoch 126/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1189 - acc: 0.9520 - val_loss: 0.1813 - val_acc: 0.9356\n",
      "Epoch 127/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1186 - acc: 0.9537 - val_loss: 0.1839 - val_acc: 0.9344\n",
      "Epoch 128/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1193 - acc: 0.9512 - val_loss: 0.1809 - val_acc: 0.9331\n",
      "Epoch 129/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1207 - acc: 0.9524 - val_loss: 0.1804 - val_acc: 0.9369\n",
      "Epoch 130/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.1173 - acc: 0.9507 - val_loss: 0.1894 - val_acc: 0.9279\n",
      "Epoch 131/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1188 - acc: 0.9503 - val_loss: 0.1834 - val_acc: 0.9331\n",
      "Epoch 132/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1198 - acc: 0.9520 - val_loss: 0.1815 - val_acc: 0.9369\n",
      "Epoch 133/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1204 - acc: 0.9512 - val_loss: 0.1843 - val_acc: 0.9331\n",
      "Epoch 134/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.1179 - acc: 0.9546 - val_loss: 0.1801 - val_acc: 0.9318\n",
      "Epoch 135/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1243 - acc: 0.9452 - val_loss: 0.1808 - val_acc: 0.9344\n",
      "Epoch 136/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1189 - acc: 0.9533 - val_loss: 0.1800 - val_acc: 0.9331\n",
      "Epoch 137/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1177 - acc: 0.9524 - val_loss: 0.1890 - val_acc: 0.9292\n",
      "Epoch 138/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1198 - acc: 0.9520 - val_loss: 0.1803 - val_acc: 0.9369\n",
      "Epoch 139/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1176 - acc: 0.9546 - val_loss: 0.1802 - val_acc: 0.9318\n",
      "Epoch 140/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1198 - acc: 0.9499 - val_loss: 0.2051 - val_acc: 0.9189\n",
      "Epoch 141/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1206 - acc: 0.9512 - val_loss: 0.1811 - val_acc: 0.9356\n",
      "Epoch 142/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1176 - acc: 0.9529 - val_loss: 0.1816 - val_acc: 0.9344\n",
      "Epoch 143/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1172 - acc: 0.9512 - val_loss: 0.1812 - val_acc: 0.9369\n",
      "Epoch 144/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1173 - acc: 0.9533 - val_loss: 0.1803 - val_acc: 0.9369\n",
      "Epoch 145/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1188 - acc: 0.9520 - val_loss: 0.1830 - val_acc: 0.9344\n",
      "Epoch 146/200\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1183 - acc: 0.9533 - val_loss: 0.1825 - val_acc: 0.9318\n",
      "Epoch 147/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1208 - acc: 0.9512 - val_loss: 0.1928 - val_acc: 0.9254\n",
      "Epoch 148/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1202 - acc: 0.9499 - val_loss: 0.1795 - val_acc: 0.9356\n",
      "Epoch 149/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1183 - acc: 0.9512 - val_loss: 0.1866 - val_acc: 0.9318\n",
      "Epoch 150/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1225 - acc: 0.9494 - val_loss: 0.2037 - val_acc: 0.9215\n",
      "Epoch 151/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1218 - acc: 0.9503 - val_loss: 0.1821 - val_acc: 0.9305\n",
      "Epoch 152/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1231 - acc: 0.9499 - val_loss: 0.1903 - val_acc: 0.9292\n",
      "Epoch 153/200\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1256 - acc: 0.9490 - val_loss: 0.2140 - val_acc: 0.9151\n",
      "Epoch 154/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1302 - acc: 0.9439 - val_loss: 0.1801 - val_acc: 0.9356\n",
      "Epoch 155/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1194 - acc: 0.9512 - val_loss: 0.1843 - val_acc: 0.9331\n",
      "Epoch 156/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1191 - acc: 0.9537 - val_loss: 0.1849 - val_acc: 0.9331\n",
      "Epoch 157/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1173 - acc: 0.9529 - val_loss: 0.1916 - val_acc: 0.9292\n",
      "Epoch 158/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1182 - acc: 0.9520 - val_loss: 0.1807 - val_acc: 0.9356\n",
      "Epoch 159/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1178 - acc: 0.9494 - val_loss: 0.1888 - val_acc: 0.9292\n",
      "Epoch 160/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1179 - acc: 0.9542 - val_loss: 0.1817 - val_acc: 0.9331\n",
      "Epoch 161/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1247 - acc: 0.9503 - val_loss: 0.1908 - val_acc: 0.9279\n",
      "Epoch 162/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1221 - acc: 0.9494 - val_loss: 0.1893 - val_acc: 0.9266\n",
      "Epoch 163/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1179 - acc: 0.9537 - val_loss: 0.1850 - val_acc: 0.9331\n",
      "Epoch 164/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.1171 - acc: 0.9533 - val_loss: 0.2007 - val_acc: 0.9189\n",
      "Epoch 165/200\n",
      "2334/2334 [==============================] - 0s 57us/step - loss: 0.1181 - acc: 0.9546 - val_loss: 0.1799 - val_acc: 0.9344\n",
      "Epoch 166/200\n",
      "2334/2334 [==============================] - 0s 54us/step - loss: 0.1209 - acc: 0.9512 - val_loss: 0.1806 - val_acc: 0.9292\n",
      "Epoch 167/200\n",
      "2334/2334 [==============================] - 0s 55us/step - loss: 0.1197 - acc: 0.9482 - val_loss: 0.1852 - val_acc: 0.9344\n",
      "Epoch 168/200\n",
      "2334/2334 [==============================] - 0s 53us/step - loss: 0.1176 - acc: 0.9524 - val_loss: 0.1816 - val_acc: 0.9344\n",
      "Epoch 169/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1169 - acc: 0.9520 - val_loss: 0.1824 - val_acc: 0.9356\n",
      "Epoch 170/200\n",
      "2334/2334 [==============================] - 0s 52us/step - loss: 0.1175 - acc: 0.9529 - val_loss: 0.1810 - val_acc: 0.9344\n",
      "Epoch 171/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1169 - acc: 0.9537 - val_loss: 0.1833 - val_acc: 0.9344\n",
      "Epoch 172/200\n",
      "2334/2334 [==============================] - 0s 54us/step - loss: 0.1169 - acc: 0.9516 - val_loss: 0.1802 - val_acc: 0.9331\n",
      "Epoch 173/200\n",
      "2334/2334 [==============================] - 0s 54us/step - loss: 0.1186 - acc: 0.9524 - val_loss: 0.1826 - val_acc: 0.9331\n",
      "Epoch 174/200\n",
      "2334/2334 [==============================] - 0s 52us/step - loss: 0.1250 - acc: 0.9452 - val_loss: 0.1823 - val_acc: 0.9344\n",
      "Epoch 175/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1166 - acc: 0.9533 - val_loss: 0.1814 - val_acc: 0.9344\n",
      "Epoch 176/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1175 - acc: 0.9520 - val_loss: 0.1825 - val_acc: 0.9356\n",
      "Epoch 177/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1174 - acc: 0.9542 - val_loss: 0.1842 - val_acc: 0.9305\n",
      "Epoch 178/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1167 - acc: 0.9559 - val_loss: 0.1823 - val_acc: 0.9356\n",
      "Epoch 179/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.1192 - acc: 0.9537 - val_loss: 0.1821 - val_acc: 0.9356\n",
      "Epoch 180/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1187 - acc: 0.9516 - val_loss: 0.1799 - val_acc: 0.9344\n",
      "Epoch 181/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.1163 - acc: 0.9542 - val_loss: 0.1951 - val_acc: 0.9305\n",
      "Epoch 182/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.1221 - acc: 0.9490 - val_loss: 0.1976 - val_acc: 0.9241\n",
      "Epoch 183/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.1216 - acc: 0.9512 - val_loss: 0.1826 - val_acc: 0.9331\n",
      "Epoch 184/200\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.1200 - acc: 0.9494 - val_loss: 0.1814 - val_acc: 0.9331\n",
      "Epoch 185/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1204 - acc: 0.9524 - val_loss: 0.1819 - val_acc: 0.9344\n",
      "Epoch 186/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.1183 - acc: 0.9537 - val_loss: 0.1829 - val_acc: 0.9356\n",
      "Epoch 187/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.1163 - acc: 0.9529 - val_loss: 0.1811 - val_acc: 0.9356\n",
      "Epoch 188/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1168 - acc: 0.9537 - val_loss: 0.1863 - val_acc: 0.9344\n",
      "Epoch 189/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.1166 - acc: 0.9512 - val_loss: 0.1837 - val_acc: 0.9356\n",
      "Epoch 190/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1165 - acc: 0.9524 - val_loss: 0.1809 - val_acc: 0.9318\n",
      "Epoch 191/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.1176 - acc: 0.9546 - val_loss: 0.1816 - val_acc: 0.9369\n",
      "Epoch 192/200\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.1151 - acc: 0.9529 - val_loss: 0.1818 - val_acc: 0.9305\n",
      "Epoch 193/200\n",
      "2334/2334 [==============================] - 0s 54us/step - loss: 0.1186 - acc: 0.9520 - val_loss: 0.1844 - val_acc: 0.9344\n",
      "Epoch 194/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.1263 - acc: 0.9482 - val_loss: 0.1919 - val_acc: 0.9305\n",
      "Epoch 195/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.1227 - acc: 0.9524 - val_loss: 0.1839 - val_acc: 0.9266\n",
      "Epoch 196/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.1241 - acc: 0.9456 - val_loss: 0.1855 - val_acc: 0.9331\n",
      "Epoch 197/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1176 - acc: 0.9533 - val_loss: 0.1825 - val_acc: 0.9331\n",
      "Epoch 198/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.1153 - acc: 0.9542 - val_loss: 0.1823 - val_acc: 0.9356\n",
      "Epoch 199/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.1176 - acc: 0.9529 - val_loss: 0.1803 - val_acc: 0.9331\n",
      "Epoch 200/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.1164 - acc: 0.9533 - val_loss: 0.1864 - val_acc: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fd23ac8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtr_best, ytr, epochs=200, batch_size=100, validation_data=(xts_best, yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing samples for best Feature Combo= 0.93564993565\n"
     ]
    }
   ],
   "source": [
    "score, acc2 = model.evaluate(xts_best, yts, verbose=0)\n",
    "print(\"Accuracy on testing samples for best Feature Combo=\", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Worst Feature Combo from subset regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_worst = np.array(df_train[\"age65plus\"])\n",
    "xts_worst = np.array(df_test[\"age65plus\"])\n",
    "\n",
    "xtr_worst = (xtr_worst - np.mean(xtr_worst, axis=0))/np.std(xtr_worst, axis=0)\n",
    "xts_worst = (xts_worst - np.mean(xts_worst, axis=0))/np.std(xts_worst, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 1000)              2000      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 4,002\n",
      "Trainable params: 4,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "nh=1000             #number of hidden units\n",
    "N = 1 #number of features, aka number of input neurons\n",
    "M = 2             #number of output neurons. \n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(N,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(M, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2334 samples, validate on 777 samples\n",
      "Epoch 1/200\n",
      "2334/2334 [==============================] - 0s 211us/step - loss: 0.4707 - acc: 0.8213 - val_loss: 0.4704 - val_acc: 0.8314\n",
      "Epoch 2/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.4038 - acc: 0.8470 - val_loss: 0.4196 - val_acc: 0.8301\n",
      "Epoch 3/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.3875 - acc: 0.8470 - val_loss: 0.4357 - val_acc: 0.8314\n",
      "Epoch 4/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3851 - acc: 0.8479 - val_loss: 0.4316 - val_acc: 0.8211\n",
      "Epoch 5/200\n",
      "2334/2334 [==============================] - 0s 57us/step - loss: 0.3900 - acc: 0.8470 - val_loss: 0.4368 - val_acc: 0.8301\n",
      "Epoch 6/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3875 - acc: 0.8488 - val_loss: 0.4159 - val_acc: 0.8185\n",
      "Epoch 7/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3856 - acc: 0.8428 - val_loss: 0.4148 - val_acc: 0.8237\n",
      "Epoch 8/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3825 - acc: 0.8445 - val_loss: 0.4169 - val_acc: 0.8263\n",
      "Epoch 9/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3851 - acc: 0.8445 - val_loss: 0.4252 - val_acc: 0.8263\n",
      "Epoch 10/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3845 - acc: 0.8436 - val_loss: 0.4187 - val_acc: 0.8250\n",
      "Epoch 11/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3838 - acc: 0.8462 - val_loss: 0.4158 - val_acc: 0.8185\n",
      "Epoch 12/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3807 - acc: 0.8428 - val_loss: 0.4184 - val_acc: 0.8263\n",
      "Epoch 13/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3821 - acc: 0.8445 - val_loss: 0.4304 - val_acc: 0.8288\n",
      "Epoch 14/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3829 - acc: 0.8492 - val_loss: 0.4222 - val_acc: 0.8224\n",
      "Epoch 15/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3800 - acc: 0.8483 - val_loss: 0.4153 - val_acc: 0.8198\n",
      "Epoch 16/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3804 - acc: 0.8458 - val_loss: 0.4154 - val_acc: 0.8237\n",
      "Epoch 17/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3816 - acc: 0.8500 - val_loss: 0.4223 - val_acc: 0.8250\n",
      "Epoch 18/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3810 - acc: 0.8470 - val_loss: 0.4697 - val_acc: 0.8301\n",
      "Epoch 19/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3869 - acc: 0.8479 - val_loss: 0.4441 - val_acc: 0.8121\n",
      "Epoch 20/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3906 - acc: 0.8410 - val_loss: 0.4163 - val_acc: 0.8198\n",
      "Epoch 21/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3830 - acc: 0.8440 - val_loss: 0.4155 - val_acc: 0.8198\n",
      "Epoch 22/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3824 - acc: 0.8440 - val_loss: 0.4374 - val_acc: 0.8288\n",
      "Epoch 23/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3849 - acc: 0.8500 - val_loss: 0.4170 - val_acc: 0.8185\n",
      "Epoch 24/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.3836 - acc: 0.8470 - val_loss: 0.4177 - val_acc: 0.8198\n",
      "Epoch 25/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.3822 - acc: 0.8453 - val_loss: 0.4175 - val_acc: 0.8198\n",
      "Epoch 26/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3813 - acc: 0.8458 - val_loss: 0.4161 - val_acc: 0.8198\n",
      "Epoch 27/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3805 - acc: 0.8449 - val_loss: 0.4183 - val_acc: 0.8263\n",
      "Epoch 28/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3866 - acc: 0.8436 - val_loss: 0.4205 - val_acc: 0.8263\n",
      "Epoch 29/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3868 - acc: 0.8466 - val_loss: 0.4335 - val_acc: 0.8288\n",
      "Epoch 30/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3830 - acc: 0.8428 - val_loss: 0.4151 - val_acc: 0.8237\n",
      "Epoch 31/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3841 - acc: 0.8449 - val_loss: 0.4155 - val_acc: 0.8185\n",
      "Epoch 32/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3820 - acc: 0.8449 - val_loss: 0.4155 - val_acc: 0.8250\n",
      "Epoch 33/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3891 - acc: 0.8440 - val_loss: 0.4292 - val_acc: 0.8288\n",
      "Epoch 34/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3831 - acc: 0.8432 - val_loss: 0.4156 - val_acc: 0.8211\n",
      "Epoch 35/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3869 - acc: 0.8449 - val_loss: 0.4294 - val_acc: 0.8263\n",
      "Epoch 36/200\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.3891 - acc: 0.8428 - val_loss: 0.4181 - val_acc: 0.8198\n",
      "Epoch 37/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3803 - acc: 0.8449 - val_loss: 0.4250 - val_acc: 0.8263\n",
      "Epoch 38/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.3825 - acc: 0.8475 - val_loss: 0.4165 - val_acc: 0.8250\n",
      "Epoch 39/200\n",
      "2334/2334 [==============================] - 0s 48us/step - loss: 0.3851 - acc: 0.8453 - val_loss: 0.4176 - val_acc: 0.8198\n",
      "Epoch 40/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3837 - acc: 0.8449 - val_loss: 0.4189 - val_acc: 0.8211\n",
      "Epoch 41/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3845 - acc: 0.8449 - val_loss: 0.4157 - val_acc: 0.8198\n",
      "Epoch 42/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3841 - acc: 0.8445 - val_loss: 0.4188 - val_acc: 0.8211\n",
      "Epoch 43/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3895 - acc: 0.8423 - val_loss: 0.4162 - val_acc: 0.8198\n",
      "Epoch 44/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3858 - acc: 0.8453 - val_loss: 0.4190 - val_acc: 0.8211\n",
      "Epoch 45/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3839 - acc: 0.8440 - val_loss: 0.4172 - val_acc: 0.8263\n",
      "Epoch 46/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3843 - acc: 0.8449 - val_loss: 0.4161 - val_acc: 0.8198\n",
      "Epoch 47/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3819 - acc: 0.8475 - val_loss: 0.4197 - val_acc: 0.8250\n",
      "Epoch 48/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3802 - acc: 0.8458 - val_loss: 0.4176 - val_acc: 0.8198\n",
      "Epoch 49/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3820 - acc: 0.8470 - val_loss: 0.4150 - val_acc: 0.8198\n",
      "Epoch 50/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3850 - acc: 0.8449 - val_loss: 0.4153 - val_acc: 0.8198\n",
      "Epoch 51/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3801 - acc: 0.8479 - val_loss: 0.4155 - val_acc: 0.8198\n",
      "Epoch 52/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.3835 - acc: 0.8479 - val_loss: 0.4161 - val_acc: 0.8250\n",
      "Epoch 53/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3897 - acc: 0.8423 - val_loss: 0.4333 - val_acc: 0.8288\n",
      "Epoch 54/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.3827 - acc: 0.8440 - val_loss: 0.4163 - val_acc: 0.8250\n",
      "Epoch 55/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3842 - acc: 0.8462 - val_loss: 0.4164 - val_acc: 0.8198\n",
      "Epoch 56/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3834 - acc: 0.8475 - val_loss: 0.4224 - val_acc: 0.8224\n",
      "Epoch 57/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3820 - acc: 0.8475 - val_loss: 0.4215 - val_acc: 0.8250\n",
      "Epoch 58/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3836 - acc: 0.8436 - val_loss: 0.4157 - val_acc: 0.8237\n",
      "Epoch 59/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3929 - acc: 0.8423 - val_loss: 0.4291 - val_acc: 0.8263\n",
      "Epoch 60/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3912 - acc: 0.8462 - val_loss: 0.4186 - val_acc: 0.8211\n",
      "Epoch 61/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3855 - acc: 0.8445 - val_loss: 0.4218 - val_acc: 0.8250\n",
      "Epoch 62/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3827 - acc: 0.8453 - val_loss: 0.4163 - val_acc: 0.8250\n",
      "Epoch 63/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3829 - acc: 0.8453 - val_loss: 0.4228 - val_acc: 0.8224\n",
      "Epoch 64/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3818 - acc: 0.8432 - val_loss: 0.4173 - val_acc: 0.8250\n",
      "Epoch 65/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3807 - acc: 0.8458 - val_loss: 0.4149 - val_acc: 0.8211\n",
      "Epoch 66/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3790 - acc: 0.8449 - val_loss: 0.4365 - val_acc: 0.8288\n",
      "Epoch 67/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3903 - acc: 0.8423 - val_loss: 0.4228 - val_acc: 0.8263\n",
      "Epoch 68/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3838 - acc: 0.8475 - val_loss: 0.4505 - val_acc: 0.8134\n",
      "Epoch 69/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3888 - acc: 0.8402 - val_loss: 0.4167 - val_acc: 0.8237\n",
      "Epoch 70/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3853 - acc: 0.8445 - val_loss: 0.4173 - val_acc: 0.8250\n",
      "Epoch 71/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3898 - acc: 0.8410 - val_loss: 0.4396 - val_acc: 0.8288\n",
      "Epoch 72/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.4007 - acc: 0.8398 - val_loss: 0.4387 - val_acc: 0.8288\n",
      "Epoch 73/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3990 - acc: 0.8419 - val_loss: 0.4285 - val_acc: 0.8237\n",
      "Epoch 74/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3919 - acc: 0.8428 - val_loss: 0.4165 - val_acc: 0.8198\n",
      "Epoch 75/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3929 - acc: 0.8423 - val_loss: 0.4290 - val_acc: 0.8263\n",
      "Epoch 76/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3827 - acc: 0.8470 - val_loss: 0.4192 - val_acc: 0.8263\n",
      "Epoch 77/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3818 - acc: 0.8483 - val_loss: 0.4164 - val_acc: 0.8250\n",
      "Epoch 78/200\n",
      "2334/2334 [==============================] - 0s 47us/step - loss: 0.3792 - acc: 0.8466 - val_loss: 0.4171 - val_acc: 0.8263\n",
      "Epoch 79/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3832 - acc: 0.8479 - val_loss: 0.4164 - val_acc: 0.8250\n",
      "Epoch 80/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3808 - acc: 0.8458 - val_loss: 0.4465 - val_acc: 0.8301\n",
      "Epoch 81/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.4005 - acc: 0.8436 - val_loss: 0.4578 - val_acc: 0.8301\n",
      "Epoch 82/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3966 - acc: 0.8496 - val_loss: 0.4178 - val_acc: 0.8250\n",
      "Epoch 83/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3886 - acc: 0.8419 - val_loss: 0.4172 - val_acc: 0.8198\n",
      "Epoch 84/200\n",
      "2334/2334 [==============================] - 0s 46us/step - loss: 0.3857 - acc: 0.8436 - val_loss: 0.4195 - val_acc: 0.8211\n",
      "Epoch 85/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3830 - acc: 0.8488 - val_loss: 0.4153 - val_acc: 0.8198\n",
      "Epoch 86/200\n",
      "2334/2334 [==============================] - 0s 45us/step - loss: 0.3845 - acc: 0.8440 - val_loss: 0.4160 - val_acc: 0.8185\n",
      "Epoch 87/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3824 - acc: 0.8470 - val_loss: 0.4155 - val_acc: 0.8198\n",
      "Epoch 88/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3849 - acc: 0.8453 - val_loss: 0.4156 - val_acc: 0.8237\n",
      "Epoch 89/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3791 - acc: 0.8453 - val_loss: 0.4240 - val_acc: 0.8250\n",
      "Epoch 90/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3823 - acc: 0.8453 - val_loss: 0.4157 - val_acc: 0.8250\n",
      "Epoch 91/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3800 - acc: 0.8449 - val_loss: 0.4248 - val_acc: 0.8250\n",
      "Epoch 92/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3892 - acc: 0.8453 - val_loss: 0.4238 - val_acc: 0.8224\n",
      "Epoch 93/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3854 - acc: 0.8445 - val_loss: 0.4233 - val_acc: 0.8250\n",
      "Epoch 94/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3801 - acc: 0.8462 - val_loss: 0.4203 - val_acc: 0.8211\n",
      "Epoch 95/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3800 - acc: 0.8466 - val_loss: 0.4151 - val_acc: 0.8198\n",
      "Epoch 96/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3826 - acc: 0.8458 - val_loss: 0.4161 - val_acc: 0.8250\n",
      "Epoch 97/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3802 - acc: 0.8466 - val_loss: 0.4152 - val_acc: 0.8198\n",
      "Epoch 98/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3874 - acc: 0.8423 - val_loss: 0.4154 - val_acc: 0.8198\n",
      "Epoch 99/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3896 - acc: 0.8453 - val_loss: 0.4758 - val_acc: 0.8301\n",
      "Epoch 100/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3874 - acc: 0.8462 - val_loss: 0.4161 - val_acc: 0.8198\n",
      "Epoch 101/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3819 - acc: 0.8445 - val_loss: 0.4157 - val_acc: 0.8250\n",
      "Epoch 102/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3858 - acc: 0.8423 - val_loss: 0.4179 - val_acc: 0.8198\n",
      "Epoch 103/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3857 - acc: 0.8436 - val_loss: 0.4146 - val_acc: 0.8237\n",
      "Epoch 104/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3827 - acc: 0.8445 - val_loss: 0.4298 - val_acc: 0.8275\n",
      "Epoch 105/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3816 - acc: 0.8449 - val_loss: 0.4265 - val_acc: 0.8263\n",
      "Epoch 106/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3809 - acc: 0.8458 - val_loss: 0.4223 - val_acc: 0.8263\n",
      "Epoch 107/200\n",
      "2334/2334 [==============================] - 0s 43us/step - loss: 0.3794 - acc: 0.8470 - val_loss: 0.4316 - val_acc: 0.8288\n",
      "Epoch 108/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3854 - acc: 0.8432 - val_loss: 0.4284 - val_acc: 0.8275\n",
      "Epoch 109/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3856 - acc: 0.8449 - val_loss: 0.4150 - val_acc: 0.8211\n",
      "Epoch 110/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3856 - acc: 0.8436 - val_loss: 0.4390 - val_acc: 0.8288\n",
      "Epoch 111/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3873 - acc: 0.8445 - val_loss: 0.4269 - val_acc: 0.8263\n",
      "Epoch 112/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.3822 - acc: 0.8406 - val_loss: 0.4309 - val_acc: 0.8275\n",
      "Epoch 113/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3855 - acc: 0.8458 - val_loss: 0.4156 - val_acc: 0.8250\n",
      "Epoch 114/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3822 - acc: 0.8445 - val_loss: 0.4179 - val_acc: 0.8198\n",
      "Epoch 115/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3798 - acc: 0.8458 - val_loss: 0.4169 - val_acc: 0.8185\n",
      "Epoch 116/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3815 - acc: 0.8432 - val_loss: 0.4186 - val_acc: 0.8250\n",
      "Epoch 117/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3813 - acc: 0.8466 - val_loss: 0.4248 - val_acc: 0.8237\n",
      "Epoch 118/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3827 - acc: 0.8483 - val_loss: 0.4224 - val_acc: 0.8250\n",
      "Epoch 119/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3919 - acc: 0.8440 - val_loss: 0.4165 - val_acc: 0.8198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3867 - acc: 0.8440 - val_loss: 0.4172 - val_acc: 0.8185\n",
      "Epoch 121/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3903 - acc: 0.8436 - val_loss: 0.4297 - val_acc: 0.8250\n",
      "Epoch 122/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3786 - acc: 0.8466 - val_loss: 0.4179 - val_acc: 0.8263\n",
      "Epoch 123/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3797 - acc: 0.8458 - val_loss: 0.4174 - val_acc: 0.8198\n",
      "Epoch 124/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3848 - acc: 0.8458 - val_loss: 0.4157 - val_acc: 0.8185\n",
      "Epoch 125/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3799 - acc: 0.8453 - val_loss: 0.4285 - val_acc: 0.8263\n",
      "Epoch 126/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3872 - acc: 0.8475 - val_loss: 0.4157 - val_acc: 0.8250\n",
      "Epoch 127/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3811 - acc: 0.8423 - val_loss: 0.4159 - val_acc: 0.8250\n",
      "Epoch 128/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.3853 - acc: 0.8449 - val_loss: 0.4387 - val_acc: 0.8288\n",
      "Epoch 129/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3864 - acc: 0.8449 - val_loss: 0.4200 - val_acc: 0.8263\n",
      "Epoch 130/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3791 - acc: 0.8453 - val_loss: 0.4241 - val_acc: 0.8250\n",
      "Epoch 131/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3878 - acc: 0.8462 - val_loss: 0.4171 - val_acc: 0.8185\n",
      "Epoch 132/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3850 - acc: 0.8432 - val_loss: 0.4152 - val_acc: 0.8211\n",
      "Epoch 133/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3789 - acc: 0.8449 - val_loss: 0.4177 - val_acc: 0.8263\n",
      "Epoch 134/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3837 - acc: 0.8466 - val_loss: 0.4299 - val_acc: 0.8288\n",
      "Epoch 135/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3846 - acc: 0.8440 - val_loss: 0.4213 - val_acc: 0.8263\n",
      "Epoch 136/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3801 - acc: 0.8479 - val_loss: 0.4151 - val_acc: 0.8211\n",
      "Epoch 137/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3877 - acc: 0.8466 - val_loss: 0.4386 - val_acc: 0.8288\n",
      "Epoch 138/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3801 - acc: 0.8415 - val_loss: 0.4206 - val_acc: 0.8250\n",
      "Epoch 139/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3829 - acc: 0.8449 - val_loss: 0.4430 - val_acc: 0.8121\n",
      "Epoch 140/200\n",
      "2334/2334 [==============================] - 0s 38us/step - loss: 0.3888 - acc: 0.8402 - val_loss: 0.4237 - val_acc: 0.8263\n",
      "Epoch 141/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3953 - acc: 0.8449 - val_loss: 0.4498 - val_acc: 0.8301\n",
      "Epoch 142/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3904 - acc: 0.8440 - val_loss: 0.4184 - val_acc: 0.8250\n",
      "Epoch 143/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.4010 - acc: 0.8359 - val_loss: 0.4159 - val_acc: 0.8185\n",
      "Epoch 144/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3804 - acc: 0.8458 - val_loss: 0.4150 - val_acc: 0.8211\n",
      "Epoch 145/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3847 - acc: 0.8449 - val_loss: 0.4151 - val_acc: 0.8237\n",
      "Epoch 146/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3835 - acc: 0.8462 - val_loss: 0.4439 - val_acc: 0.8288\n",
      "Epoch 147/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3849 - acc: 0.8436 - val_loss: 0.4154 - val_acc: 0.8198\n",
      "Epoch 148/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3807 - acc: 0.8470 - val_loss: 0.4360 - val_acc: 0.8288\n",
      "Epoch 149/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3865 - acc: 0.8440 - val_loss: 0.4249 - val_acc: 0.8263\n",
      "Epoch 150/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3826 - acc: 0.8462 - val_loss: 0.4180 - val_acc: 0.8263\n",
      "Epoch 151/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3824 - acc: 0.8466 - val_loss: 0.4209 - val_acc: 0.8250\n",
      "Epoch 152/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3822 - acc: 0.8445 - val_loss: 0.4170 - val_acc: 0.8263\n",
      "Epoch 153/200\n",
      "2334/2334 [==============================] - 0s 42us/step - loss: 0.3817 - acc: 0.8462 - val_loss: 0.4220 - val_acc: 0.8224\n",
      "Epoch 154/200\n",
      "2334/2334 [==============================] - 0s 40us/step - loss: 0.3878 - acc: 0.8440 - val_loss: 0.4149 - val_acc: 0.8211\n",
      "Epoch 155/200\n",
      "2334/2334 [==============================] - 0s 39us/step - loss: 0.3821 - acc: 0.8483 - val_loss: 0.4289 - val_acc: 0.8263\n",
      "Epoch 156/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3845 - acc: 0.8470 - val_loss: 0.4640 - val_acc: 0.8301\n",
      "Epoch 157/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3933 - acc: 0.8462 - val_loss: 0.4160 - val_acc: 0.8250\n",
      "Epoch 158/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3800 - acc: 0.8458 - val_loss: 0.4150 - val_acc: 0.8211\n",
      "Epoch 159/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3811 - acc: 0.8445 - val_loss: 0.4460 - val_acc: 0.8301\n",
      "Epoch 160/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3860 - acc: 0.8432 - val_loss: 0.4304 - val_acc: 0.8263\n",
      "Epoch 161/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3815 - acc: 0.8496 - val_loss: 0.4161 - val_acc: 0.8198\n",
      "Epoch 162/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3819 - acc: 0.8419 - val_loss: 0.4631 - val_acc: 0.8301\n",
      "Epoch 163/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3888 - acc: 0.8440 - val_loss: 0.4264 - val_acc: 0.8250\n",
      "Epoch 164/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3828 - acc: 0.8462 - val_loss: 0.4152 - val_acc: 0.8198\n",
      "Epoch 165/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3843 - acc: 0.8479 - val_loss: 0.4155 - val_acc: 0.8198\n",
      "Epoch 166/200\n",
      "2334/2334 [==============================] - 0s 44us/step - loss: 0.3848 - acc: 0.8462 - val_loss: 0.4208 - val_acc: 0.8211\n",
      "Epoch 167/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3833 - acc: 0.8466 - val_loss: 0.4204 - val_acc: 0.8263\n",
      "Epoch 168/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3784 - acc: 0.8453 - val_loss: 0.4176 - val_acc: 0.8263\n",
      "Epoch 169/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3808 - acc: 0.8436 - val_loss: 0.4185 - val_acc: 0.8263\n",
      "Epoch 170/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3836 - acc: 0.8449 - val_loss: 0.4163 - val_acc: 0.8250\n",
      "Epoch 171/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3820 - acc: 0.8462 - val_loss: 0.4315 - val_acc: 0.8288\n",
      "Epoch 172/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3882 - acc: 0.8419 - val_loss: 0.4159 - val_acc: 0.8237\n",
      "Epoch 173/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3810 - acc: 0.8428 - val_loss: 0.4178 - val_acc: 0.8198\n",
      "Epoch 174/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3837 - acc: 0.8470 - val_loss: 0.4154 - val_acc: 0.8250\n",
      "Epoch 175/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3886 - acc: 0.8410 - val_loss: 0.4255 - val_acc: 0.8263\n",
      "Epoch 176/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3826 - acc: 0.8445 - val_loss: 0.4156 - val_acc: 0.8185\n",
      "Epoch 177/200\n",
      "2334/2334 [==============================] - 0s 28us/step - loss: 0.3840 - acc: 0.8449 - val_loss: 0.4156 - val_acc: 0.8198\n",
      "Epoch 178/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.3848 - acc: 0.8500 - val_loss: 0.4173 - val_acc: 0.8250\n",
      "Epoch 179/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3816 - acc: 0.8436 - val_loss: 0.4159 - val_acc: 0.8185\n",
      "Epoch 180/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3861 - acc: 0.8466 - val_loss: 0.4164 - val_acc: 0.8198\n",
      "Epoch 181/200\n",
      "2334/2334 [==============================] - 0s 31us/step - loss: 0.3826 - acc: 0.8419 - val_loss: 0.4226 - val_acc: 0.8250\n",
      "Epoch 182/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3874 - acc: 0.8466 - val_loss: 0.4352 - val_acc: 0.8288\n",
      "Epoch 183/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.3864 - acc: 0.8428 - val_loss: 0.4342 - val_acc: 0.8288\n",
      "Epoch 184/200\n",
      "2334/2334 [==============================] - 0s 30us/step - loss: 0.3827 - acc: 0.8466 - val_loss: 0.4180 - val_acc: 0.8198\n",
      "Epoch 185/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3885 - acc: 0.8415 - val_loss: 0.4232 - val_acc: 0.8250\n",
      "Epoch 186/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3908 - acc: 0.8393 - val_loss: 0.4414 - val_acc: 0.8288\n",
      "Epoch 187/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3811 - acc: 0.8462 - val_loss: 0.4172 - val_acc: 0.8250\n",
      "Epoch 188/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3837 - acc: 0.8466 - val_loss: 0.4235 - val_acc: 0.8224\n",
      "Epoch 189/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3838 - acc: 0.8428 - val_loss: 0.4159 - val_acc: 0.8250\n",
      "Epoch 190/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3840 - acc: 0.8458 - val_loss: 0.4175 - val_acc: 0.8263\n",
      "Epoch 191/200\n",
      "2334/2334 [==============================] - 0s 29us/step - loss: 0.3850 - acc: 0.8488 - val_loss: 0.4458 - val_acc: 0.8288\n",
      "Epoch 192/200\n",
      "2334/2334 [==============================] - 0s 32us/step - loss: 0.3872 - acc: 0.8436 - val_loss: 0.4154 - val_acc: 0.8237\n",
      "Epoch 193/200\n",
      "2334/2334 [==============================] - 0s 37us/step - loss: 0.3811 - acc: 0.8470 - val_loss: 0.4214 - val_acc: 0.8224\n",
      "Epoch 194/200\n",
      "2334/2334 [==============================] - 0s 33us/step - loss: 0.3818 - acc: 0.8432 - val_loss: 0.4211 - val_acc: 0.8263\n",
      "Epoch 195/200\n",
      "2334/2334 [==============================] - 0s 28us/step - loss: 0.3819 - acc: 0.8445 - val_loss: 0.4234 - val_acc: 0.8263\n",
      "Epoch 196/200\n",
      "2334/2334 [==============================] - 0s 34us/step - loss: 0.3843 - acc: 0.8462 - val_loss: 0.4158 - val_acc: 0.8250\n",
      "Epoch 197/200\n",
      "2334/2334 [==============================] - 0s 35us/step - loss: 0.3824 - acc: 0.8440 - val_loss: 0.4206 - val_acc: 0.8250\n",
      "Epoch 198/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3800 - acc: 0.8462 - val_loss: 0.4158 - val_acc: 0.8198\n",
      "Epoch 199/200\n",
      "2334/2334 [==============================] - 0s 36us/step - loss: 0.3789 - acc: 0.8419 - val_loss: 0.4157 - val_acc: 0.8185\n",
      "Epoch 200/200\n",
      "2334/2334 [==============================] - 0s 41us/step - loss: 0.3815 - acc: 0.8466 - val_loss: 0.4204 - val_acc: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fe6fcf8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtr_worst, ytr, epochs=200, batch_size=100, validation_data=(xts_worst, yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing samples for best Feature Combo= 0.824967825045\n"
     ]
    }
   ],
   "source": [
    "score, acc3 = model.evaluate(xts_worst, yts, verbose=0)\n",
    "print(\"Accuracy on testing samples for best Feature Combo=\", acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting All Features, Best featuers and Worst Features training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc4AAAEKCAYAAACbn7USAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njVsyyauMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1220fba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAD8CAYAAACl69mTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMxy4yRjZma5+f8PKWRUj9TgzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12236f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh([1,2,3],[acc1,acc2,acc3],color=['red', 'blue', 'green'])\n",
    "plt.yticks([1,2,3], [\"All Features\", \"Top Feature Comb\", \"Worst Feature Comb\"])\n",
    "plt.xlabel(\"Accuracy\")\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(111)\n",
    "ax.plot([\"All Features\",\"Top Features\",\"Worst Feature\"],[acc1,acc2,acc3], color='orange')\n",
    "ax.scatter([\"All Features\",\"Top Features\",\"Worst Feature\"],[acc1,acc2,acc3], color=['red', 'blue', 'green'])\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
